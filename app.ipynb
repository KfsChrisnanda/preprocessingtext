{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f483bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\neptune\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 lxml pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv\n",
    "from glob import iglob\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
    "SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
    "URL_RE = re.compile(r'https?://|www\\.', re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}')\n",
    "DIGIT_RE = re.compile(r'\\d')\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    if not text:\n",
    "        return []\n",
    "    return SENT_SPLIT_RE.split(text)\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "def is_story_like(s: str) -> bool:\n",
    "    s = s.strip()\n",
    "    if not s: return False\n",
    "    if URL_RE.search(s): return False\n",
    "    if EMAIL_RE.search(s): return False\n",
    "    if DIGIT_RE.search(s): return False\n",
    "    caps_tokens = sum(1 for t in s.split() if len(t) > 2 and t.isupper())\n",
    "    if caps_tokens >= 3: return False\n",
    "    return True\n",
    "\n",
    "def _process_file(path: str, filter_non_story: bool = True):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            raw = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skip {path}: {e}\")\n",
    "        return []\n",
    "    sentences = split_sentences(raw)\n",
    "    out = []\n",
    "    for s in sentences:\n",
    "        if filter_non_story and not is_story_like(s):\n",
    "            continue\n",
    "        toks = tokenize(s)\n",
    "        if len(toks) == 5:\n",
    "            out.append({\n",
    "                \"sentence_raw\": s.strip(),\n",
    "                \"sentence_clean\": \" \".join(toks),\n",
    "                \"tokens\": \" \".join(toks),\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def _iter_txt_files(p: str):\n",
    "    if os.path.isfile(p):\n",
    "        if p.lower().endswith(\".txt\"):\n",
    "            yield p\n",
    "        else:\n",
    "            print(f\"[WARN] Not a .txt: {p}\")\n",
    "        return\n",
    "    for q in iglob(os.path.join(p, \"**\", \"*.txt\"), recursive=True):\n",
    "        yield q\n",
    "\n",
    "def _load_seen_keys(csv_path: str, key_col: str):\n",
    "    \"\"\"Muat set nilai kolom 'key_col' dari CSV (untuk de-dup).\"\"\"\n",
    "    seen = set()\n",
    "    if not os.path.isfile(csv_path):\n",
    "        return seen\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                k = row.get(key_col)\n",
    "                if k:\n",
    "                    seen.add(k)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Gagal baca CSV untuk de-dup: {e}\")\n",
    "    return seen\n",
    "\n",
    "def preprocess_5word_sentences(input_path: str,\n",
    "                               output_csv: str = \"five_word_sentences.csv\",\n",
    "                               filter_non_story: bool = True,\n",
    "                               append: bool = True,\n",
    "                               dedupe: bool = True,\n",
    "                               dedupe_key: str = \"sentence_clean\"):\n",
    "    rows, nfiles = [], 0\n",
    "    for fp in _iter_txt_files(input_path):\n",
    "        nfiles += 1\n",
    "        rows.extend(_process_file(fp, filter_non_story=filter_non_story))\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    file_exists = os.path.isfile(output_csv)\n",
    "    mode = \"a\" if (append and file_exists) else \"w\"\n",
    "\n",
    "    \n",
    "    with open(output_csv, mode, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        fieldnames = [\"sentence_raw\", \"sentence_clean\", \"tokens\"]\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if mode == \"w\":\n",
    "            w.writeheader()\n",
    "\n",
    "        # siapkan set untuk de-dup\n",
    "        seen = _load_seen_keys(output_csv, dedupe_key) if (dedupe and file_exists) else set()\n",
    "\n",
    "        written = 0\n",
    "        for r in rows:\n",
    "            if dedupe:\n",
    "                k = r.get(dedupe_key)\n",
    "                if k in seen:\n",
    "                    continue\n",
    "                seen.add(k)\n",
    "            w.writerow(r)\n",
    "            written += 1\n",
    "\n",
    "    print(f\"[DONE] files={nfiles}  extracted(5 kata)={len(rows)}  written={written}  saved={output_csv}\")\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fda47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] files=1  extracted(5 kata)=86  written=86  saved=C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\n",
      "[DONE] files=1  extracted(5 kata)=55  written=55  saved=C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\n",
      "[DONE] files=1  extracted(5 kata)=75  written=75  saved=C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\n",
      "[DONE] files=1  extracted(5 kata)=115  written=115  saved=C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\n"
     ]
    }
   ],
   "source": [
    "rows = preprocess_5word_sentences(\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\The Hound of the Baskervilles.txt\",\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\"\n",
    ")\n",
    "\n",
    "rows = preprocess_5word_sentences(\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\A Study In Scarlet.txt\",\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\"\n",
    ")\n",
    "\n",
    "rows = preprocess_5word_sentences(\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\The Sign of the Four.txt\",\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\"\n",
    ")\n",
    "rows = preprocess_5word_sentences(\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\The Valley of Fear.txt\",\n",
    "    r\"C:\\Users\\Neptune\\Documents\\tugas_kefas\\Semester 5\\NLP\\ancog\\five_word_sentences.csv\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
